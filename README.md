# BERT vs ModernBERT Comparison

IMDB ì˜í™” ë¦¬ë·° ê°ì • ë¶„ì„ì„ ìœ„í•œ BERTì™€ ModernBERT ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

- **ë°ì´í„°ì…‹**: IMDB ì˜í™” ë¦¬ë·° (ê¸ì •/ë¶€ì • ê°ì • ë¶„ë¥˜)
- **ëª¨ë¸**: BERT-base-uncased vs answerdotai/ModernBERT-base
- **ì‹¤í—˜ ì¶”ì **: Weights & Biases (WandB)
- **í™˜ê²½**: Python 3.9, PyTorch, Transformers

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
exp_1/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py          # ë©”ì¸ ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ model.py         # ë¶„ë¥˜ ëª¨ë¸ ì •ì˜
â”‚   â”œâ”€â”€ data.py          # ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ (ì‚¬ì „ í† í¬ë‚˜ì´ì§•)
â”‚   â””â”€â”€ utils.py         # ì„¤ì • ë¡œë”© ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ configs.yaml     # ëª¨ë¸, ë°ì´í„°, í›ˆë ¨ ì„¤ì •
â”œâ”€â”€ wandb/              # WandB ì‹¤í—˜ ë¡œê·¸ (gitì—ì„œ ì œì™¸)
â””â”€â”€ README.md
```

## ğŸš€ ì£¼ìš” íŠ¹ì§•

### 1. **ì‚¬ì „ í† í¬ë‚˜ì´ì§• (Pre-tokenization)**
- í›ˆë ¨ ì¤‘ í† í¬ë‚˜ì´ì§• ì˜¤ë²„í—¤ë“œ ì™„ì „ ì œê±°
- ë¹ ë¥¸ ë°ì´í„° ë¡œë”©ìœ¼ë¡œ í›ˆë ¨ ì†ë„ í–¥ìƒ

### 2. **ê°„ê²°í•œ ì½”ë“œ ì„¤ê³„**
- í•µì‹¬ ê¸°ëŠ¥ë§Œ í¬í•¨ëœ ìµœì†Œí•œì˜ ì½”ë“œ
- ëª…í™•í•œ íƒ€ì… íŒíŠ¸ì™€ docstring
- í•œ ì¤„ `__getitem__` êµ¬í˜„

### 3. **ì„¤ì • ê¸°ë°˜ ì‹¤í—˜ ê´€ë¦¬**
- OmegaConfë¥¼ í™œìš©í•œ YAML ì„¤ì • íŒŒì¼
- ì½”ë“œ ìˆ˜ì • ì—†ì´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³€ê²½ ê°€ëŠ¥
- ëª¨ë¸ë³„ ë…ë¦½ì ì¸ ì„¤ì • ê´€ë¦¬

### 4. **ìë™í™”ëœ ì‹¤í—˜ ë¹„êµ**
- ë‘ ëª¨ë¸ ìˆœì°¨ì  í›ˆë ¨ ë° ë¹„êµ
- WandBë¥¼ í†µí•œ ì‹¤ì‹œê°„ ì‹¤í—˜ ì¶”ì 
- ê²€ì¦ ì„±ëŠ¥ ìë™ ë¡œê¹…

## âš™ï¸ ì„¤ì¹˜ ë° ì‹¤í–‰

### í™˜ê²½ ì„¤ì •
```bash
# Python 3.9 ê°€ìƒí™˜ê²½ ìƒì„±
python3.9 -m venv venv39
source venv39/bin/activate

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch tqdm wandb datasets numpy omegaconf "transformers>=4.46.0"
```

### ì‹¤í—˜ ì‹¤í–‰
```bash
cd exp_1
python src/main.py
```

## ğŸ“Š ì‹¤í—˜ ì„¤ì •

### ëª¨ë¸ ì„¤ì •
- **BERT**: bert-base-uncased (768 hidden size)
- **ModernBERT**: answerdotai/ModernBERT-base (768 hidden size)

### ë°ì´í„° ì„¤ì •
- **ìµœëŒ€ ê¸¸ì´**: 128 í† í°
- **ë°°ì¹˜ í¬ê¸°**: 32
- **ë¶„í• **: Train(45,000) / Valid(2,500) / Test(2,500)

### í›ˆë ¨ ì„¤ì •
- **ì—í¬í¬**: 5
- **í•™ìŠµë¥ **: 5e-5
- **ì˜µí‹°ë§ˆì´ì €**: Adam
- **ì†ì‹¤ í•¨ìˆ˜**: CrossEntropyLoss

## ğŸ¯ í•µì‹¬ êµ¬í˜„ ì‚¬í•­

### 1. ì‚¬ì „ í† í¬ë‚˜ì´ì§• ë°ì´í„°ì…‹
```python
def __getitem__(self, idx: int) -> dict:
    return {key: self.dataset[key][idx] for key in self.dataset}
```

### 2. ê°„ê²°í•œ ëª¨ë¸ ì •ì˜
```python
def forward(self, input_ids, attention_mask, label, token_type_ids=None):
    outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
    hidden = outputs.last_hidden_state 
    pooled = hidden[:, 0, :]  # CLS í† í° ì‚¬ìš©
    logits = self.classifier(pooled)
    loss = self.loss_fn(logits, label)
    return {'logits': logits, 'loss': loss}
```

### 3. íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬
```python
@staticmethod
def collate_fn(batch: List[dict]) -> dict:
    keys = batch[0].keys()
    return {key: torch.stack([sample[key] for sample in batch]) for key in keys}
```

## ğŸ“ˆ ì‹¤í—˜ ê²°ê³¼

WandB ëŒ€ì‹œë³´ë“œì—ì„œ ë‹¤ìŒ ë©”íŠ¸ë¦­ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- í›ˆë ¨/ê²€ì¦ ì†ì‹¤
- ê²€ì¦ ì •í™•ë„
- ì—í¬í¬ë³„ ì„±ëŠ¥ ë¹„êµ
- ëª¨ë¸ë³„ ìˆ˜ë ´ ì†ë„

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

- **Python**: 3.9+
- **PyTorch**: 2.8.0
- **Transformers**: 4.55.2 (ModernBERT ì§€ì›)
- **Datasets**: 4.0.0 (IMDB ë°ì´í„° ë¡œë”©)
- **WandB**: 0.21.1 (ì‹¤í—˜ ì¶”ì )
- **OmegaConf**: 2.3.0 (ì„¤ì • ê´€ë¦¬)


