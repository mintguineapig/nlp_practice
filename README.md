# BERT vs ModernBERT Comparison

IMDB ì˜í™” ë¦¬ë·° ê°ì • ë¶„ì„ì„ ìœ„í•œ BERTì™€ ModernBERT ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

- **ë°ì´í„°ì…‹**: IMDB ì˜í™” ë¦¬ë·° (ê¸ì •/ë¶€ì • ê°ì • ë¶„ë¥˜)
- **ëª¨ë¸**: BERT-base-uncased vs answerdotai/ModernBERT-base
- **ì‹¤í—˜ ì¶”ì **: Weights & Biases (WandB)
- **í™˜ê²½**: Python 3.9, PyTorch, Transformers

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
exp_1/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py          # ë©”ì¸ ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ model.py         # ë¶„ë¥˜ ëª¨ë¸ ì •ì˜
â”‚   â”œâ”€â”€ data.py          # ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ (ì‚¬ì „ í† í¬ë‚˜ì´ì§•)
â”‚   â””â”€â”€ utils.py         # ì„¤ì • ë¡œë”© ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ configs.yaml     # ëª¨ë¸, ë°ì´í„°, í›ˆë ¨ ì„¤ì •
â”œâ”€â”€ wandb/              # WandB ì‹¤í—˜ ë¡œê·¸ (gitì—ì„œ ì œì™¸)
â””â”€â”€ README.md
```


### í™˜ê²½ ì„¤ì •
```bash
# Python 3.9 ê°€ìƒí™˜ê²½ ìƒì„±
python3.9 -m venv venv39
source venv39/bin/activate

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch tqdm wandb datasets numpy omegaconf "transformers>=4.46.0"
```

### ì‹¤í—˜ ì‹¤í–‰
```bash
cd exp_1
python src/main.py
```

## ğŸ“Š ì‹¤í—˜ ì„¤ì •

### ëª¨ë¸ ì„¤ì •
- **BERT**: bert-base-uncased (768 hidden size)
- **ModernBERT**: answerdotai/ModernBERT-base (768 hidden size)

### ë°ì´í„° ì„¤ì •
- **ìµœëŒ€ ê¸¸ì´**: 128 í† í°
- **ë°°ì¹˜ í¬ê¸°**: 32
- **ë¶„í• **: Train(45,000) / Valid(2,500) / Test(2,500)

### í›ˆë ¨ ì„¤ì •
- **ì—í¬í¬**: 5
- **í•™ìŠµë¥ **: 5e-5
- **ì˜µí‹°ë§ˆì´ì €**: Adam
- **ì†ì‹¤ í•¨ìˆ˜**: CrossEntropyLoss

## ğŸ¯ í•µì‹¬ êµ¬í˜„ ì‚¬í•­

### 1. ì‚¬ì „ í† í¬ë‚˜ì´ì§• ë°ì´í„°ì…‹
```python
def __getitem__(self, idx: int) -> dict:
    return {key: self.dataset[idx][key] for key in self.dataset}
```

### 2. ê°„ê²°í•œ ëª¨ë¸ ì •ì˜
```python
def forward(self, input_ids, attention_mask, label, token_type_ids=None):
    outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
    hidden = outputs.last_hidden_state 
    pooled = hidden[:, 0, :]  # CLS í† í° ì‚¬ìš©
    logits = self.classifier(pooled)
    loss = self.loss_fn(logits, label)
    return {'logits': logits, 'loss': loss}
```

### 3. íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬
```python
@staticmethod
def collate_fn(batch: List[dict]) -> dict:
    keys = batch[0].keys()
    return {key: torch.stack([sample[key] for sample in batch]) for key in keys}
```

## ğŸ“ˆ ì‹¤í—˜ ê²°ê³¼

<img width="481" height="389" alt="image" src="https://github.com/user-attachments/assets/b641d387-eba9-4785-8efc-37f789da8d60" />

| ëª¨ë¸        | ì •í™•ë„(Accuracy) |
|-------------|------------------|
| ModernBERT  | 0.90902          |
| BERT        | 0.89241          |


modernBERTì˜ ì„±ëŠ¥ì´ BERTì˜ ì„±ëŠ¥ë³´ë‹¤ ë” ì¢‹ë‹¤. 

modernBERT ëŠ” BERT ì™€ ìœ ì‚¬í•œ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ê°–ì§€ë§Œ, ë¯¸ì„¸í•˜ê²Œ í† í¬ë‚˜ì´ì €ì™€ ê°™ì€ ì„¸ë¶€ì ì¸ ì•„í‚¤í…ì²˜ê°€ ìµœì í™”ë˜ì–´ìˆê¸° ë•Œë¬¸ì´ë‹¤. 

(ì¶œì²˜ : https://jina.ai/ko/news/what-should-we-learn-from-modernbert/ )

ì´ë¡œì„œ scaling law ë¡œ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒ ë¿ë§Œ ì•„ë‹ˆë¼ ì•„í‚¤í…ì²˜ ìµœì í™”ë„ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ í•„ìš”í•˜ë‹¤ëŠ” ì ì„ ì•Œ ìˆ˜ ìˆë‹¤.


## ì‹¤í—˜ ê²°ê³¼ - Batch Size ë° ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ

| ë°°ì¹˜ í¬ê¸° | ëª¨ë¸ | ì‹¤í–‰ ì‹œê°„(ì´ˆ) | ì •í™•ë„ |
|:--------:|:----:|:------------:|:------:|
| 64 | ModernBERT | 591 | 0.90273 |
| 64 | BERT | 416 | 0.87539 |
| 256 | ModernBERT | 563 | 0.91523 |
| 256 | BERT | 393 | 0.89687 |
| 1024 | ModernBERT | 548 | 0.83984 |
| 1024 | BERT | 386 | 0.89297 |

- ë™ì¼ batch size ì— ëŒ€í•˜ì—¬ accumulator ì„ ì‚¬ìš©í–ˆì„ ë•Œ í•™ìŠµ ì†ë„ê°€ ë” ë¹¨ëë‹¤.

- Batch size ê°€ ì¦ê°€í• ìˆ˜ë¡ ì‹¤í–‰ ì‹œê°„(ì´ˆ) ëŠ” ê°ì†Œí•˜ì˜€ìœ¼ë‚˜, ì •í™•ë„ëŠ” ì˜¤íˆë ¤ ì‹¤í–‰ ìˆœì„œì— ë¹„ë¡€í–ˆë‹¤.

- ì¶”ê°€ ì‹¤í—˜ : ê¸°ì¡´(1024 -> 64 -> 256) ê³¼ ë°˜ëŒ€ë¡œ ì‹¤í–‰




## ğŸ§‘ğŸ¼â€ğŸ’» ì½”ë“œ ì‘ì„± ì‹œ ì£¼ì˜í•  ì 
1. Configs ë¥¼ í™œìš©í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì¡°ì ˆ

2. input, output ìë£Œí˜•ì„ í•¨ìˆ˜ì— ëª…ì‹œí•˜ì—¬ ì˜¤ë¥˜ë¥¼ ì˜ˆë°©

3. ì†ë„ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ì‚¬ì „ tokenizing ì„ ìˆ˜í–‰

4. Reproductivity ë¥¼ ìœ„í•œ seed ê³ ì • (seed = 42)

5. wandb API í‚¤ëŠ” ë³¸ì¸ì˜ key ë¡œ ëŒ€ì²´í•´ì•¼ í•©ë‹ˆë‹¤. 

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

- **Python**: 3.9+
- **PyTorch**: 2.8.0
- **Transformers**: 4.55.2 (ModernBERT ì§€ì›)
- **Datasets**: 4.0.0 (IMDB ë°ì´í„° ë¡œë”©)
- **WandB**: 0.21.1 (ì‹¤í—˜ ì¶”ì )
- **OmegaConf**: 2.3.0 (ì„¤ì • ê´€ë¦¬)


